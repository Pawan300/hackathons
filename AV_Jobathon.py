# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ig9qdLhHvQ6K-TAGN2OQtKuCh8KQZ-Ps
"""

import pandas as pd
import numpy as np


import matplotlib.pyplot as plt
import seaborn as sbn
import plotly.subplots as sub
import plotly.graph_objs as go
import plotly.express as px

test_data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Analytic Vidhya/Job-a-thon/dataset/test_mSzZ8RL.csv")
train = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Analytic Vidhya/Job-a-thon/dataset/train_s3TEQDk.csv")

print("train shape : {}\ntest shape : {}".format(train.shape, test_data.shape))

train.isnull().sum()

train.head()

temp = train.groupby(["Gender", "Is_Lead"]).count()["ID"].reset_index()

fig = px.bar(temp, x="Gender", y="ID", color="Is_Lead", title="Label distribution according to Gender")
fig.show()

temp = train.groupby(["Occupation", "Is_Lead"]).count()["ID"].reset_index()

fig = px.bar(temp, x="Occupation", y="ID", color="Is_Lead", title="Label distribution according to Occupation")
fig.show()

train.groupby(
    "Occupation"
).mean()["Avg_Account_Balance"].reset_index()

# train = train.drop("ID", axis =1)
# ids = test_data["ID"]
# test_data = test_data.drop("ID", axis =1)

gender = {
    "Female" : 1,
    "Male" : -1
}
train["Gender"] = list(map(lambda x : gender[x], train["Gender"]))
test_data["Gender"] = list(map(lambda x : gender[x], test_data["Gender"]))

Region = {}
for i,j in enumerate(train["Region_Code"].unique()):
  Region[j] = i
train["Region_Code"] = list(map(lambda x : Region[x], train["Region_Code"]))
test_data["Region_Code"] = list(map(lambda x : Region[x], test_data["Region_Code"]))

Occupation = {
    "Other" : 1,
    "Salaried" : 2,
    "Self_Employed" : 3,
    "Entrepreneur" : 4
}
train["Occupation"] = list(map(lambda x : Occupation[x], train["Occupation"]))
test_data["Occupation"] = list(map(lambda x : Occupation[x], test_data["Occupation"]))

channel = {
    "X1" : 1,
    "X2" : 2,
    "X3" : 3,
    "X4" : 4
}
train["Channel_Code"] = list(map(lambda x : channel[x], train["Channel_Code"]))
test_data["Channel_Code"] = list(map(lambda x : channel[x], test_data["Channel_Code"]))

Is_Active = {
    "Yes" : 1,
    "No" : -1
}
train["Is_Active"] = list(map(lambda x : Is_Active[x], train["Is_Active"]))
test_data["Is_Active"] = list(map(lambda x : Is_Active[x], test_data["Is_Active"]))

train = train.fillna(method="ffill")
test_data = test_data.fillna(method="ffill")

Credit_Product = {
    "Yes" : 1,
    "No" : -1
}
train["Credit_Product"] = list(map(lambda x : Credit_Product[x], train["Credit_Product"]))
test_data["Credit_Product"] = list(map(lambda x : Credit_Product[x], test_data["Credit_Product"]))

train["Avg_Account_Balance"] = (train["Avg_Account_Balance"] - train["Avg_Account_Balance"].mean()) / train["Avg_Account_Balance"].std() 
test_data["Avg_Account_Balance"] = (test_data["Avg_Account_Balance"] - test_data["Avg_Account_Balance"].mean()) / test_data["Avg_Account_Balance"].std()

# Age : above 60 : 1 
#       60-40    : 2
#       40-25    : 3
#       below 25 : 4
def age(data):
  return list(map(lambda x : 1 if x>60 else( 2 if 60>x>40 else (3 if 40>x>25 else 4)), data)) 


train["Age"] = age(train["Age"])
test_data["Age"] = age(test_data["Age"])

train.head()

train.groupby(
    ["Vintage", "Is_Lead"]
).count()["ID"].reset_index()

train.corr()

train.groupby(
    ["Occupation", "Is_Lead"]
).agg({
    "ID" : 'count',
})

train['Avg_Account_Balance_log'] = np.log(train['Avg_Account_Balance'])
test['Avg_Account_Balance_log'] = np.log(test['Avg_Account_Balance'])

train['Vintage_log'] = np.log(train['Vintage'])
test['Vintage_log'] = np.log(test['Vintage'])


from sklearn.preprocessing import KBinsDiscretizer
est = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='quantile')
est.fit(train['Avg_Account_Balance'].values.reshape(-1,1))
train['Avg_Account_Balance_quantiles'] = est.transform(train['Avg_Account_Balance'].values.reshape(-1,1)).astype(int)
test['Avg_Account_Balance_quantiles'] = est.transform(test['Avg_Account_Balance'].values.reshape(-1,1)).astype(int)

from sklearn.preprocessing import KBinsDiscretizer
est = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='quantile')
est.fit(train['Vintage'].values.reshape(-1,1))
train['Vintage_quantiles'] = est.transform(train['Vintage'].values.reshape(-1,1)).astype(int)
test['Vintage_quantiles'] = est.transform(test['Vintage'].values.reshape(-1,1)).astype(int)

from sklearn.preprocessing import KBinsDiscretizer
est = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='quantile')
est.fit(train['Age'].values.reshape(-1,1))
train['Age_quantiles'] = est.transform(train['Age'].values.reshape(-1,1)).astype(int)
test['Age_quantiles'] = est.transform(test['Age'].values.reshape(-1,1)).astype(int)

from scipy import stats
fitted_data, fitted_lambda = stats.boxcox(np.abs(train['Vintage']))
df_train_final['Vintage_box'] = fitted_data
fitted_data, fitted_lambda = stats.boxcox(np.abs(test['Vintage']))
df_test_final['Vintage_box'] = fitted_data

sbn.heatmap(train.corr())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc

temp = train["Is_Lead"]
train = train.drop("Is_Lead", axis=1)

Xtrain, Xtest, ytrain, ytest = train_test_split(train, temp, train_size = 0.8)

Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape

clf = RandomForestClassifier()

parameters = {'n_estimators': [4, 6, 9], 
              'max_features': ['log2', 'sqrt','auto'], 
              'criterion': ['entropy', 'gini'],
              'max_depth': [2, 3, 5, 10], 
              'min_samples_split': [2, 3, 5],
              'min_samples_leaf': [1,5,8]
             }

from sklearn.metrics import make_scorer, accuracy_score
from sklearn.model_selection import GridSearchCV

acc_scorer = make_scorer(accuracy_score)

grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)
grid_obj = grid_obj.fit(Xtrain, ytrain)

clf = grid_obj.best_estimator_

clf.fit(X_train, y_train)

# @title Error
def plot_roc_curve(fprs, tprs):
    """Plot the Receiver Operating Characteristic from a list
    of true positive rates and false positive rates."""
    
    # Initialize useful lists + the plot axes.
    tprs_interp = []
    aucs = []
    mean_fpr = np.linspace(0, 1, 100)
    f, ax = plt.subplots(figsize=(14,10))
    
    # Plot ROC for each K-Fold + compute AUC scores.
    for i, (fpr, tpr) in enumerate(zip(fprs, tprs)):
        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))
        tprs_interp[-1][0] = 0.0
        roc_auc = auc(fpr, tpr)
        aucs.append(roc_auc)
        ax.plot(fpr, tpr, lw=1, alpha=0.3,
                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))
        
    # Plot the luck line.
    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
             label='Luck', alpha=.8)
    
    # Plot the mean ROC.
    mean_tpr = np.mean(tprs_interp, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    ax.plot(mean_fpr, mean_tpr, color='b',
             label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
             lw=2, alpha=.8)
    
    # Plot the standard deviation around the mean ROC.
    std_tpr = np.std(tprs_interp, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
                     label=r'$\pm$ 1 std. dev.')
    
    # Fine tune and show the plot.
    ax.set_xlim([-0.05, 1.05])
    ax.set_ylim([-0.05, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver operating characteristic')
    ax.legend(loc="lower right")
    plt.show()
    return (f, ax)
    
def compute_roc_auc(index):
    y_predict = clf.predict_proba(Xtrain.iloc[index])[:,1]
    fpr, tpr, thresholds = roc_curve(ytrain.iloc[index], y_predict)
    auc_score = auc(fpr, tpr)
    return fpr, tpr, auc_score

cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)
results = pd.DataFrame(columns=['training_score', 'test_score'])
fprs, tprs, scores = [], [], []
    
for (train, test), i in zip(cv.split(Xtrain, ytrain), range(5)):
    clf.fit(Xtrain.iloc[train], ytrain.iloc[train])
    _, _, auc_score_train = compute_roc_auc(train)
    fpr, tpr, auc_score = compute_roc_auc(test)
    scores.append((auc_score_train, auc_score))
    fprs.append(fpr)
    tprs.append(tpr)

plot_roc_curve(fprs, tprs);
pd.DataFrame(scores, columns=['AUC Train', 'AUC Test'])

# @title XGBOOST
from xgboost import XGBRegressor
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from xgboost import XGBClassifier

params = {
        'min_child_weight': [1, 5, 10]
        ,
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
}
folds = 3
param_comb = 5

skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)
xgb =XGBRegressor(n_estimators=1000, learning_rate=0.05)
random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, 
                                   cv=skf.split(Xtrain,ytrain), verbose=3, random_state=1001 )

# Here we go
def timer(start_time=None):
    if not start_time:
        start_time = datetime.now()
        return start_time
    elif start_time:
        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)
        tmin, tsec = divmod(temp_sec, 60)
        print('\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))

start_time = timer(None) # timing starts from this point for "start_time" variable
random_search.fit(Xtrain, ytrain)
timer(start_time)

data = {
    "ID" : ids,
    "Is_Lead" : sol
}
result = pd.DataFrame(data)
result.to_csv("/content/drive/MyDrive/Colab Notebooks/Analytic Vidhya/Job-a-thon/dataset/solution.csv", index=False)

# @title ROC Score
from sklearn.metrics import roc_auc_score
def roc(ytest, ypred):
  return "ROC\nMicro : {}\nMacro : {}".format(roc_auc_score(ytest, ypred, average="micro"), roc_auc_score(ytest, ypred, average="macro"))

roc(ytest, random_search.predict(Xtest))

# @title LGBM Classifier

from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import roc_auc_score

def cross_val(X, y, model, params, folds=9):

    skf = StratifiedKFold(n_splits=folds, shuffle=True)
    roc_scores_macro = []
    roc_scores_micro = []
    prediction = []
    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):
        print(f"Fold: {fold}")
        x_train, y_train = X[train_idx], y[train_idx]
        x_test, y_test = X[test_idx], y[test_idx]

        alg = model(**params)
        alg.fit(x_train, y_train,
                eval_set=[(x_test, y_test)],
                early_stopping_rounds=100,
                verbose=400)

        pred = alg.predict_proba(x_test)[:, 1]
        roc_score_macro = roc_auc_score(y_test, pred, average='macro')
        roc_score_micro = roc_auc_score(y_test, pred, average='micro')
        roc_scores_macro.append(roc_score_macro)
        roc_scores_micro.append(roc_score_micro)
        print("Macro : ",roc_score_macro)
        print("Micro : ",roc_score_micro)
        prediction.append(alg.predict_proba(X_test)[:,1])
        print("-"*50)
    print("Avg ROC micro : ", sum(roc_scores_micro)/folds)
    print("Avg ROC macro : ", sum(roc_scores_macro)/folds)
    return alg, prediction


lgb_params= {'learning_rate': 0.45, 
             'n_estimators': 2000, 
             'max_bin': 40,
             'num_leaves': 10,  
             'reg_lambda': 0.532, 
             'subsample': 0.749}
from lightgbm import LGBMClassifier
lgb_model, prediction = cross_val(X, y, LGBMClassifier, lgb_params,5)
pd.DataFrame({"ID":ids, "Is_Lead":np.mean(prediction, axis=0)}).to_csv("lgb2.csv", index=False)

